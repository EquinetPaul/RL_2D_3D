{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784ccf9d-a138-441e-803c-a7f1fe63da7d",
   "metadata": {},
   "source": [
    "# Définition de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15868af-911e-4ea3-a9bb-f6bcdc647d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GridEnvironment:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (10, 10)\n",
    "        self.obstacle_positions = [(2,2), (2,3), (2,4), (5,5), (6,5), (7,5)]\n",
    "        self.goal_position = (9,9)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_position = (0,0)\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, y = self.agent_position\n",
    "        if action == 0: # up\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 1: # down\n",
    "            x = min(x + 1, self.grid_size[0] - 1)\n",
    "        elif action == 2: # left\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 3: # right\n",
    "            y = min(y + 1, self.grid_size[1] - 1)\n",
    "        \n",
    "        if (x, y) in self.obstacle_positions:\n",
    "            reward = -10\n",
    "        elif (x, y) == self.goal_position:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        self.agent_position = (x, y)\n",
    "        done = (x, y) == self.goal_position\n",
    "        return self.agent_position, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e0dc0-655e-430a-93a8-5282035d281c",
   "metadata": {},
   "source": [
    "# Définition de la Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d0e48c-fd05-4d55-8793-b488f8026c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.q_table = np.random.rand(*state_size, action_size)\n",
    "        \n",
    "    def get_action(self, state, epsilon):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return np.random.choice(len(self.q_table[state]))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, alpha, gamma):\n",
    "        q_value = reward + gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] = (1 - alpha) * self.q_table[state][action] + alpha * q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2971db-8df3-4737-89e3-01315e2ebe58",
   "metadata": {},
   "source": [
    "# Entraînement de la Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e072b2-ae5b-4dbd-9631-d7f2001e1845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement de la Q-Table\n"
     ]
    }
   ],
   "source": [
    "print(\"Entraînement de la Q-Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d92066f-6da9-45d6-9839-5a0cea6554fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1300.76it/s]\n"
     ]
    }
   ],
   "source": [
    "env = GridEnvironment()\n",
    "q_table = QTable(state_size=env.grid_size, action_size=4)\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "epsilon = 0.1 # exploration rate\n",
    "alpha = 0.5 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    env.reset()\n",
    "    state = env.agent_position\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        action = q_table.get_action(state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        q_table.update(state, action, reward, next_state, alpha, gamma)\n",
    "        state = next_state\n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38ec1c-e940-4bdb-a989-271e0048ccd0",
   "metadata": {},
   "source": [
    "# Définition de la fonction de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fa1d44-0a18-40aa-9930-7ca0aac09e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.9.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "def visualize(q_table, env, num_episodes, max_steps):\n",
    "    pygame.init()\n",
    "\n",
    "    grid_size = env.grid_size\n",
    "    screen_width = 400\n",
    "    screen_height = 400\n",
    "    screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "    font = pygame.font.Font(None, 30)\n",
    "\n",
    "    cell_width = screen_width // grid_size[1]\n",
    "    cell_height = screen_height // grid_size[0]\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.agent_position\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = q_table.get_action(state, epsilon=0)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "            screen.fill((255, 255, 255))\n",
    "            for row in range(grid_size[0]):\n",
    "                for col in range(grid_size[1]):\n",
    "                    if (row, col) in env.obstacle_positions:\n",
    "                        color = (0, 0, 0)\n",
    "                    elif (row, col) == env.goal_position:\n",
    "                        color = (0, 255, 0)\n",
    "                    else:\n",
    "                        color = (255, 255, 255)\n",
    "                    pygame.draw.rect(screen, color, (col * cell_width, row * cell_height, cell_width, cell_height))\n",
    "\n",
    "                    pygame.draw.line(screen, (0, 0, 0), (0, row * cell_height), (screen_width, row * cell_height), 1)\n",
    "                    pygame.draw.line(screen, (0, 0, 0), (col * cell_width, 0), (col * cell_width, screen_height), 1)\n",
    "            pygame.draw.circle(screen, (255, 0, 0), (env.agent_position[1] * cell_width + cell_width // 2, env.agent_position[0] * cell_height + cell_height // 2), min(cell_width, cell_height) // 2)\n",
    "            text = font.render(f\"Step: {steps}\", True, (0, 0, 0))\n",
    "            screen.blit(text, (10, 10))\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            pygame.display.update()\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f9a73-1454-4f17-b8cf-36e10ab13b8e",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29d5375-f707-4216-8ed1-643054c2677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(q_table, env, num_episodes=100, max_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
